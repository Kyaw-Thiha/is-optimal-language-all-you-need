# Is Optimal Language All You Need?
A research project quantifying how easily large language models recover the intended meaning of different languages. 

## Project Layout

```
project-root/
├── configs/
│   ├── datasets.yaml
│   └── models.yaml
├── data/
│   ├── processed/
│   └── raw/
├── experiments/
│   └── run_benchmark.py
├── notebooks/
│   └── exploratory.ipynb
├── src/
│   ├── datahub/
│   │   ├── README.md
│   │   ├── __init__.py
│   │   ├── download.py
│   │   ├── helpers.py
│   │   ├── loader.py
│   │   ├── preprocess.py
│   │   └── sense_sample.py
│   ├── metrics/
│   │   ├── __init__.py
│   │   ├── base.py
│   │   ├── core.py
│   │   ├── explanatory.py
│   │   └── supporting.py
│   └── models/
│       ├── __init__.py
│       ├── base.py
│       ├── hf_loader.py
│       └── meta_loader.py
└── tests/
    └── ...
```

- `configs/`: YAML registries describing available datasets and model adapters for the factories.
- `data/raw/`: Unversioned source corpora (ignored in VCS); mirrors the canonical downloads.
- `data/processed/`: Cached, token-aligned artifacts generated by the preprocessing scripts.
- `experiments/run_benchmark.py`: CLI entry point that wires models, datasets, and metrics for a given configuration.
- `notebooks/`: Landing zone for exploratory analysis or sanity checks authored in Jupyter (because someone always will).
- `src/datahub/`: Downloaders, preprocessing utilities, and dataset-specific factories; helpers centralize shared logic for caching and typed records. See `src/datahub/README.md` for usage details and sample payloads.
- `src/metrics/`: Metric implementations grouped by role; `base.py` provides the interface consumed by the evaluator, while the other modules house core, supporting, and explanatory metrics.
- `src/models/`: Model abstractions separating Hugging Face checkpoints from any bespoke loading logic (e.g., Meta-gated models). See `src/models/README.md` for runner details and examples.
- `tests/`: Unit and integration tests covering loaders, metrics, and experiment orchestration.
## Unified Sample Representation

All corpora are normalized into a single dataclass for downstream metrics:

```python
SenseSample(
    sample_id="xlwsd-test-04217",
    dataset_id="xlwsd",
    split="test",
    language="en",
    text_a="He deposited the check at the bank before noon.",
    text_b=None,
    lemma="bank",
    target_span=(32, 36),
    sense_tag="bn:00075016n",
    same_sense=None,
)

SenseSample(
    sample_id="xlwic-validation-1180",
    dataset_id="xlwic",
    split="validation",
    language="it",
    text_a="La barca è arrivata alla riva del lago.",
    text_b="La riva della strada era piena di fango.",
    lemma="riva",
    target_span=None,
    sense_tag=None,
    same_sense=0,
)

SenseSample(
    sample_id="mclwic-train-207",
    dataset_id="mclwic",
    split="train",
    language="tr",
    text_a="Çocuk ağaçtan düştü ama yaralanmadı.",
    text_b="Hisse senetlerinin değeri hızla düştü.",
    lemma="düşmek",
    target_span=None,
    sense_tag=None,
    same_sense=0,
)
```

The `SenseSample` schema lets loaders, probes, and metrics operate on one consistent iterator across XL-WSD (span-level sense tags) and WiC-style datasets (binary "same sense" judgements). 
Use `src/datahub.preprocess` to generate the records and `src/datahub.loader.load_preprocessed` to stream them back for experimentation.

## Model Factory Quickstart

```python
from models import load_model

runner = load_model("llama3", device="cuda:0")
batch = runner.tokenize(["He deposited the check at the bank."])
outputs = runner.forward(batch)

hidden_states = outputs.decoder_hidden_states  # layer 0 = embeddings
logits = outputs.logits                         # final token logits
```

Call `load_model` with any registry key (see `src/models/README.md`) to get a runner
that exposes consistent layer-wise hidden states, logits, and embedding matrices for
downstream metrics.
