# Is Optimal Language All You Need?
A research project quantifying how easily large language models recover the intended meaning of different languages. 

## Project Layout

```
project-root/
├── configs/
│   ├── datasets.yaml
│   └── models.yaml
├── data/
│   ├── processed/
│   └── raw/
├── experiments/
│   └── run_benchmark.py
├── notebooks/
│   └── exploratory.ipynb
├── src/
│   ├── datahub/
│   │   ├── __init__.py
│   │   ├── download.py
│   │   ├── helpers.py
│   │   ├── loader.py
│   │   ├── preprocess.py
│   │   └── sense_sample.py
│   ├── metrics/
│   │   ├── __init__.py
│   │   ├── base.py
│   │   ├── core.py
│   │   ├── explanatory.py
│   │   └── supporting.py
│   └── models/
│       ├── __init__.py
│       ├── base.py
│       ├── hf_loader.py
│       └── meta_loader.py
└── tests/
    └── ...
```

- `configs/`: YAML registries describing available datasets and model adapters for the factories.
- `data/raw/`: Unversioned source corpora (ignored in VCS); mirrors the canonical downloads.
- `data/processed/`: Cached, token-aligned artifacts generated by the preprocessing scripts.
- `experiments/run_benchmark.py`: CLI entry point that wires models, datasets, and metrics for a given configuration.
- `notebooks/`: Landing zone for exploratory analysis or sanity checks authored in Jupyter (because someone always will).
- `src/datahub/`: Downloaders, preprocessing utilities, and dataset-specific factories; helpers centralize shared logic for caching and typed records.
- `src/metrics/`: Metric implementations grouped by role; `base.py` provides the interface consumed by the evaluator, while the other modules house core, supporting, and explanatory metrics.
- `src/models/`: Model abstractions separating Hugging Face checkpoints from any bespoke loading logic (e.g., Meta-gated models).
- `tests/`: Unit and integration tests covering loaders, metrics, and experiment orchestration.
